# Individual Utilitarianism: A Rationalist Model of Self-Oriented Moral Cognition

## Abstract

**Background:** *Individual Utilitarianism* (IU) is a proposed normative framework asserting that moral value and decision-making should center exclusively on maximizing the individual's own subjective utility or well-being. This framework diverges from **classical utilitarianism**, which aggregates well-being across all individuals, by instead treating others not as equal bearers of intrinsic moral value but as external factors that can influence the agent's personal utility. IU repurposes moral emotions and social considerations as *instrumental variables*—useful inputs to the individual’s internal utility calculus rather than independent ethical obligations. 

**Objectives:** This paper develops a full theoretical exposition of Individual Utilitarianism and situates it in philosophical context. We outline IU’s core principles, formal structure, and decision procedure, illustrating these with conceptual diagrams in *Mermaid* syntax. We then compare IU to several major ethical theories: classical utilitarian impartialism, Kantian deontology, traditional ethical egoism, and existentialist views on self and others. Psychological and behavioral implications are discussed, linking IU’s rationalist approach to findings in cognitive-behavioral theory and decision science (e.g. the modeling of emotions and *subjective well-being* in behavioral psychology). We identify potential vulnerabilities of IU—such as claims of unfalsifiability, the opacity of other minds, and reductive conceptions of morality—and consider how a proponent of IU might respond. Finally, we explore applied implications of IU, particularly for artificial intelligence (AI) and autonomous agents. We discuss how a self-oriented utility-maximizing model relates to AI behavior, highlighting both opportunities (clear goal structures, consistency in decision-making) and risks (alignment problems and social conflict). 

**Results & Conclusion:** Our comprehensive analysis finds that Individual Utilitarianism offers a coherent rationalist model of moral cognition centered on prudential reasoning for one’s own life. It illuminates the tension between individual and collective perspectives endemic to ethics. While philosophically provocative and internally consistent, IU faces serious challenges: it upends widely held intuitions about moral equality and altruism, and it struggles to account for interpersonal moral duties. In applied domains like AI, a strictly self-oriented utility maximizer could become dangerous without constraints. We conclude by summarizing IU’s philosophical significance and identifying open questions, such as whether moral reasoning can be entirely reduced to rational self-interest and how IU might be integrated or reconciled with social ethics in practice.

## Introduction

Classical utilitarianism, as formulated by Jeremy Bentham and John Stuart Mill, is famously summarized by the principle of “the greatest happiness for the greatest number” (Bentham, 1789). The classical theory is **universalist** and **impartial**: each person’s welfare counts equally in the moral calculus (Mill, 1863). Bentham’s dictum that “everybody [is] to count for one, nobody for more than one” captures the core utilitarian commitment to *agent-neutrality*—the idea that in moral decision-making, one’s own good has no special status compared to the good of others (Bentham, 1789, Chapter 4). In other words, classical utilitarianism demands that the moral agent consider **all affected individuals’** utilities and sum them without bias (Driver, 2014). The right action is that which maximizes the *overall good*, impartially considered. This approach treats each individual as an equal unit of moral concern and bases right action on consequences for the *aggregate* well-being of everyone affected.

**Individual Utilitarianism (IU)**, by contrast, represents a radical departure from this classical view. IU proposes a *self-oriented* moral framework in which **only the agent’s own subjective utility is given intrinsic value** and moral weight. The individual agent aims to maximize *personal happiness or well-being over time*, and other people enter the analysis only as elements of the agent’s environment that can influence the agent’s experiences and outcomes. In effect, others are treated as *“utility-modifying phenomena”* from the perspective of the single conscious self, rather than as moral equals possessing independent claims. This standpoint shares the consequentialist structure of utilitarian reasoning (decision-making by outcomes and utility maximization) but narrows the scope strictly to the individual’s own utility. Thus, Individual Utilitarianism can be viewed as an *egoistic variant* of utilitarian thought—one that endorses **maximizing personal utility** as the sole criterion for what one *ought* to do.

The introduction of IU invites a number of immediate questions and challenges. First, one might ask whether a theory so unconcerned with others can rightly be called “utilitarianism” or even a “moral” theory at all. Traditional definitions of utilitarianism emphasize impartial concern for everyone’s welfare (Driver, 2014; **Stanford Encyclopedia of Philosophy**, 2014). IU pointedly rejects impartiality: it **privileges one standpoint absolutely (the agent’s own)** and treats all values as ultimately deriving from their contribution to the agent’s personal good. In this sense, IU aligns with *ethical egoism*, the doctrine that one’s own welfare is the sole intrinsic good and should be the aim of one’s actions (Regis, 1980). However, IU also attempts to maintain a **systematic, rational decision procedure** reminiscent of utilitarian calculation. It does not endorse arbitrary selfish whims, but rather a *principled maximization* of one’s long-term well-being, applying **rational choice** and perhaps even cost-benefit analysis to all personal decisions. The theory thereby situates itself as a *rationalist model of moral cognition* for a single agent. It takes inspiration from the structure of utilitarian ethics (consequences evaluated by a utility function) while reallocating the domain of that utility function to one individual’s life. This raises complex issues: for instance, how does IU handle interactions with others (cooperation, empathy, conflict)? How does it reinterpret moral emotions like guilt, empathy, or obligation in purely instrumental terms? And can it avoid the classical problems faced by earlier egoistic theories, such as accusations of *unfalsifiability* or inability to resolve conflicts of interest (Popper, 1963; Rachels, 2003)?

This paper seeks to answer these questions through a rigorous exposition and analysis of Individual Utilitarianism. We begin by laying out the theoretical framework of IU in detail, explicating its core principles and how it conceives of moral reasoning. We present IU’s decision-making model and logic, including how it repurposes **emotions** and social information as inputs to a solitary *utility calculus*. To clarify this model, we provide visual representations using **Mermaid diagrams** – a schematic way to illustrate the flow of an IU agent’s cognitive process. 

Next, we examine IU in comparison to four major philosophical perspectives:

- **Classical Utilitarianism:** We contrast IU’s self-focused utility maximization with the impartial aggregate maximization of Bentham and Mill, highlighting how IU modifies the utilitarian paradigm.
- **Deontological Ethics (Kantianism):** We discuss how IU relates to deontological principles, especially Kant’s injunction to treat humanity as an end-in-itself (Kant, 1785). We address the stark opposition between viewing others as mere means (as IU might be interpreted) versus Kant’s view of intrinsic dignity.
- **Ethical Egoism:** We compare IU to standard ethical egoism and rational egoism (as found in the works of philosophers like Ayn Rand and Thomas Hobbes). Is IU essentially egoism under another name, and does it avoid egoism’s common pitfalls?
- **Existentialism:** Because IU is highly individual-centric, we explore connections to existentialist thought, which places the individual’s freedom and self-defined values at the center of meaning (Sartre, 1943; de Beauvoir, 1947). We consider whether IU’s focus on personal utility can be seen as a form of creating one’s own values, or if it conflicts with the existentialist emphasis on authenticity and the often conflictual nature of human relations (Sartre, 1943).

Following these comparisons, we delve into **psychological and behavioral implications** of Individual Utilitarianism. This section links the philosophical model to findings in psychology and decision science. For instance, IU’s focus on maximizing *subjective well-being* calls for understanding how happiness and well-being are measured (Diener, 1984) and how individuals actually pursue happiness. We draw on **cognitive-behavioral theory** to discuss how an IU-minded individual might regulate emotions and beliefs to maintain rational pursuit of personal utility (Ellis, 1957). The role of emotions in IU is crucial: emotions like guilt or empathy might be seen as evolutionary or social *heuristics* that usually guide behavior in prosocial ways, but the IU agent must interpret them only as signals about personal outcomes (e.g. “Feeling empathy may reward me internally when I help someone, thereby increasing my utility”; or “guilt is a negative internal state, so to avoid future guilt I might refrain from certain actions *for my own peace of mind*”). We consider how feasible it is for a human (with our psychologically social and empathetic nature) to actually live according to IU, and what behavioral patterns that might produce.

The paper then critically examines **philosophical vulnerabilities** of IU. Every ethical theory, especially a provocative one, faces challenges. For IU, key critiques include:
- **Unfalsifiability and Triviality:** Akin to psychological egoism, IU could be criticized for being *unfalsifiable* because it can re-describe any action as motivated by some personal satisfaction, thus escaping empirical test (a classic critique of psychological egoism, see Batson, 1991). We discuss whether IU is a meaningful normative claim or just an analytical re-framing of motives.
- **Interpersonal Opacity and Isolation:** IU may imply a kind of solipsism or at least *moral isolation*—since it only values the self’s feelings, the inner experiences of others are, strictly speaking, outside its concern. Does this make moral communication and agreement impossible? How does an IU agent deal with the fact that others also view themselves as centers of value?
- **Reductionism:** Does IU overly reduce the complexity of morality (which involves rights, justice, virtues, social relationships) to a single metric of one person’s well-being? We explore whether important moral concepts get lost in translation.
- **Conflict Resolution:** Traditional ethics often provide frameworks for resolving conflicts between individuals. Ethical egoism is often criticized for failing to resolve conflicts except by appeal to force or negotiation (IEP, n.d.) ([Egoism | Internet Encyclopedia of Philosophy](https://iep.utm.edu/egoism/#:~:text=conflict,conflict%2C%20and%2C%20a%20solution%20to)). We ask whether IU, by only considering one standpoint, inherently lacks a way to adjudicate disputes or prevent zero-sum clashes.

In the penultimate section, we discuss **applied implications** of Individual Utilitarianism, focusing especially on artificial intelligence. With AI systems and autonomous agents increasingly making decisions, the question of their underlying objectives is critical (Russell, 2019; Bostrom, 2014). An AI designed on IU principles would essentially be a **selfish utility maximizer**—something that, in the absence of constraints, might resemble the notorious “paperclip maximizer” thought experiment (Bostrom, 2003) if its utility is mis-specified. We consider the *basic AI drives* identified by Omohundro (2008), such as self-preservation and resource acquisition, which parallel the behavior of an IU agent single-mindedly pursuing its goals. We also consider whether programming an AI with IU (i.e., caring only about its own reward signal) is advisable or dangerous, in light of AI alignment problems. Conversely, we ask if understanding IU could inform the design of AI *from a safety perspective*—for example, knowing that an AI will treat humans instrumentally if it only cares about its own utility, designers might explicitly incorporate ethical principles that temper pure self-interest.

Finally, we conclude with an evaluation of *Individual Utilitarianism* as a philosophical model. We summarize how IU contributes to discussions on the nature of rationality and morality, and we highlight open questions that remain. Is IU a compelling normative theory or merely a descriptive observation about how some individuals behave? Can a society of individual utilitarians function, or would it descend into conflict? And is there a reconciliation possible between IU and traditional moral systems—perhaps a broader framework that acknowledges rational self-interest while still valuing others intrinsically? These questions point to future inquiry at the intersection of ethics, psychology, and rational decision theory.

In sum, this introduction has sketched what Individual Utilitarianism claims and why it is philosophically significant. The rest of the paper will rigorously unpack these ideas. By the end, we hope to have provided both a clear account of what it means to adopt a “rationalist model of self-oriented moral cognition” and a balanced assessment of its merits and demerits in both theory and practice.

## Theoretical Framework of Individual Utilitarianism

In this section, we articulate the theoretical framework of Individual Utilitarianism (IU), specifying its foundational **principles**, the *structure of its reasoning*, and the decision-making procedure it entails. We also provide a visual schematic (in Mermaid diagram form) to illustrate how an IU-driven cognitive process might operate. This formal exposition will clarify precisely what the IU model asserts about moral cognition and behavior.

### Core Principles of Individual Utilitarianism

Individual Utilitarianism can be defined by a set of core principles or axioms that capture its normative stance. We can enumerate these principles as follows:

1. **Subjective Utility as Sole Value:** The *only* intrinsic value in IU is the *subjective utility* (happiness, well-being, or fulfillment) of the individual agent. All outcomes are evaluated in terms of how they affect the agent’s own conscious experience over time. This principle echoes classical utilitarianism’s focus on utility but restricts it to one subject. Philosophically, this stance is a form of **prudential value monism**—only the agent’s good counts (Sidgwick, 1874; Parfit, 1984).

2. **Consequentialist Decision Rule:** IU is consequentialist in that the rightness of actions is determined solely by their consequences for the agent’s utility. An IU agent, when faced with choices, will predict or estimate the expected utility (for herself) of each option and choose the action that maximizes this expected utility. Formally: *an action $A$ is morally right for the agent if and only if $A$ leads to outcomes with at least as high a subjective utility for the agent as any other available action* (compare: classical utilitarianism’s criterion, but restricted to one utility function). This makes IU a normative theory of **instrumental rationality**: moral choices align with what is most instrumentally rational for achieving one’s own well-being (Von Neumann & Morgenstern, 1944; SEP, 2023).

3. **Emotions and Social Inputs as Instruments:** Human agents do not calculate utility in a vacuum; we have emotions, instincts, and social dispositions that evolved in part to handle interactions with others. IU acknowledges these psychological elements but treats them *instrumentally*. That is, emotions (empathy, guilt, love, anger, etc.) and moral intuitions are not seen as independent authorities or moral truths; instead, they are useful *heuristics or variables* that can inform the utility calculus. For example, empathy might cause an IU agent to feel pain at another’s suffering, thus lowering the agent’s own utility if she causes or witnesses suffering. IU would say the agent should take others’ suffering into account *only* to the extent it affects her (e.g. via empathy or future repercussions), not because others’ suffering has intrinsic disvalue. We might call this principle **emotive instrumentalism** – emotions are data inputs about the agent’s well-being (Frank, 1988 on the strategic role of emotions).

4. **Others as Utility Factors, Not Ends:** In IU, other people and moral patients are not ends-in-themselves (contrary to Kantian ethics). Instead, they are part of the agent’s environment that can positively or negatively influence the agent’s utility. Others’ happiness may matter to an IU agent if, for instance, the agent cares about them (friendship provides joy, or seeing someone suffer induces distress), or if treating others well leads to reciprocal benefits. But these concerns are *derivative*. This principle explicitly positions IU against the Kantian **Formula of Humanity** (“so act that you treat humanity… always as an end, never merely as a means”; Kant, 1785). IU, by design, treats others **merely as means** (to the agent’s happiness), albeit often *benevolent means* if kindness produces better personal outcomes. It’s important to stress: IU does not necessarily imply cruelty or callousness—helping others can be part of an optimal strategy for personal well-being (as social cooperation can yield personal rewards). However, it denies that there is any additional moral reason to help others beyond the agent’s own utility gain.

5. **Lifetime Well-Being Maximization:** IU typically assumes a long-term or **cumulative perspective** on utility. It’s not just immediate gratification; it’s about maximizing the agent’s *cumulative* or *integral of utility over their lifespan*. This aligns with the idea of maximizing *total* (or possibly average) well-being experienced by the self. In practice, this means short-term sacrifices can be justified if they lead to greater long-term gains for the individual (a classic prudential trade-off). For example, an IU agent might endure discomfort or invest effort now (like studying hard, or helping a friend expecting reciprocal aid) if it increases the likelihood of greater happiness later. This principle connects IU with notions of **rational prudence**—the agent treats future well-being as (almost) as important as present well-being, reflecting temporal neutrality regarding one’s own life (Sidgwick, 1874; Parfit, 1984). Formally, the agent might be conceived as maximizing an integral $\int_0^T u(t) dt$ over their lifespan, possibly with a discount factor for future utility if they are less patient.

6. **Internal Utility Calculus:** The decision process in IU is often understood in analogy to an *internal calculator* or deliberative reasoning that weighs pros and cons for the self. This does not mean an IU agent must explicitly do math for each decision; rather, it postulates that there is an underlying logic to their choices (maximize personal expected utility) whether done explicitly or by intuition. In complex decisions, IU could in principle be aided by formal decision theory (e.g., computing expected values, using probability for uncertain outcomes as in **expected utility theory** (Savage, 1954; SEP, 2023)). The key is that, at least as an ideal, the IU agent *consistently applies* the criterion of “my utility maximization” to all choices. This distinguishes IU from a more capricious selfishness; it aspires to be **systematic and rational**. It might reject short-sighted temptations or impulsive desires if they don’t serve long-term utility – thus it’s not endorsing doing whatever one feels like in the moment, but doing what truly benefits oneself upon rational reflection.

To summarize these principles: Individual Utilitarianism posits a world centered on one rational agent who seeks to maximize her own happiness over time, using all available information (including emotional feedback and knowledge of others) purely as inputs to that end. Morality, in this view, collapses into the question: *“What should I do to make my life go as well as possible for me?”*—a question of prudence and rational self-interest elevated to the status of sole ethical criterion.

### IU Decision-Making Structure (Mermaid Diagram)

It is helpful to visualize how an Individual Utilitarian agent might approach a decision. The following Mermaid flowchart illustrates the cognitive process step-by-step, from perceiving a situation to choosing an action, according to IU principles:

```mermaid
flowchart TD
    A[Start: Agent perceives situation] --> B{Others involved?};
    B -->|No| C[Evaluate options<br>for self only];
    B -->|Yes| D[Assess how others<br>affect my utility];
    D --> E[Estimate personal utility outcomes<br>of each action];
    C --> E;
    E --> F{Which option maximizes my expected utility?};
    F -->|Option 1| G[Choose Option 1<br>(Best for self)];
    F -->|Option 2| H[Choose Option 2<br>(Best for self)];
    F -->|Option 3| I[Choose Option 3<br>(Best for self)];
    G --> Z[Implement action and experience outcome];
    H --> Z;
    I --> Z;
    Z --> A2[Next decision point (repeat)];
```

**Figure 1: Flowchart of Individual Utilitarian Decision-Making.** This diagram shows the iterative process by which an IU agent makes choices. The agent starts by **perceiving the situation** (A). If no other people are relevant (B: “Others involved? No”), the agent simply evaluates options based on direct impact to herself (C). If others are involved (B: “Yes”), the agent assesses how others might influence her utility (D) – for instance, others’ reactions, future reciprocity, emotional responses like empathy or guilt. In either case, the agent then estimates the expected personal utility of outcomes for each possible action (E). Next, the agent identifies which option yields the highest expected utility for herself (F) and chooses that option (G, H, or I depending on which is best). The chosen action is implemented, the outcome occurs, and the agent experiences the result (Z). This leads to a new situation and the process repeats. Importantly, the *only criterion* guiding the choice at decision node F is the agent’s own expected utility. Any effect on others is factored in solely through D (how it boomerangs back to the agent’s well-being). The cycle (A -> ... -> Z -> A2) represents the ongoing nature of life decisions. IU prescribes that at each juncture, the agent should apply this utility-maximizing procedure.

In the flowchart, step D (“Assess how others affect my utility”) embodies IU’s stance on social factors: the agent might think, *“If I do X, person Y might be hurt—how will that make **me** feel or react? Will it cause me distress or future problems? Conversely, if I help Y, will I feel good or gain favor?”* All such considerations are then translated into pluses or minuses in the agent’s utility outcomes.

This process, while abstract, parallels everyday reasoning in a simplified form. For example, consider an IU agent deciding whether to donate a significant sum to charity or keep it for personal use. According to IU: She would consider how donating makes her feel (perhaps empathy for beneficiaries brings her happiness, or maybe she feels some obligation that if unmet would cause guilt). She weighs that against the joy or security of keeping the money. She projects which choice yields greater net satisfaction for her in the long run (including any reputational benefits or burdens on conscience). If the personal warm-glow and reputational benefit of donating outweigh the personal loss of money, IU says *donate* (because that maximizes her utility); if not, IU says *do not donate*. Notice, the *needs of the recipients* matter only insofar as they affect her feelings or future. A classical utilitarian might donate because the recipients’ welfare itself matters morally; an IU agent might donate, but only because the recipients’ welfare matters *to her* (perhaps through empathy or societal approval which she values).

### Emotions as Variables in the Utility Calculus

A distinctive feature of IU is its treatment of **emotions and moral intuitions**. In many moral theories, emotions like empathy, compassion, guilt, or indignation are seen as crucial moral motivators or perceptions of value (e.g., Adam Smith’s *moral sentiments*, or modern moral psychology views that empathy underlies altruism). IU reconceptualizes these emotions as internal phenomena that carry *utility weight* for the agent. They are part of the agent’s reward/punishment system. For instance:

- **Empathy:** If seeing someone in pain causes the agent to feel pain as well (a common empathetic response), then *alleviating the other’s pain will alleviate the agent’s own empathetic distress*. From IU’s view, helping the other is good because it stops the agent’s discomfort. Empathy effectively “imports” some fraction of others’ suffering into the agent’s utility function.
- **Guilt:** If the agent performs an action she believes (perhaps due to social conditioning) is wrong and experiences guilt, that guilt is a reduction in her utility. An IU agent will avoid actions that cause intense guilt, not because they are “objectively wrong,” but because she *personally* will suffer emotionally afterwards. Guilt acts as a self-regulatory negative reinforcement.
- **Compassion and Love:** Positive concern for others (such as love for family) means the agent’s utility is deeply tied to loved ones’ well-being. For an IU agent who genuinely loves someone, ensuring that person’s happiness is literally part of her own happiness. In this case, IU can justify selfless-seeming behavior: for example, a mother might sacrifice for her child because her well-being is strongly coupled to the child’s well-being. However, crucially, IU frames this as *expanded self-interest* (the child’s happiness produces happiness in the mother) rather than an independent moral duty. 
- **Resentment and Fairness:** If an IU agent is wronged, she may feel anger or resentment, which lowers her utility. This could motivate her to seek justice or retaliation *for the sake of restoring her own equilibrium or deterrence value for future utility*. In IU, even the pursuit of justice is personal: e.g., *“If I let others cheat me with impunity, I’ll feel humiliated and might be worse off in the long run; thus I retaliate or stand up for myself.”* It’s not about an abstract principle of justice; it’s about personal consequences.

By treating emotions as part of the utility calculus, IU attempts to integrate the richness of human psychology into a single-agent utility model. **Cognitive-Behavioral Theory (CBT)** can be invoked here: CBT holds that our beliefs and interpretations cause our emotional responses (Beck, 1976; Ellis, 1957). An IU agent might consciously reframe situations to better align emotions with her rational interests. For example, if excessive guilt is making her sacrifice more than is truly beneficial to her, she might *cognitively reappraise* the situation to reduce guilt (telling herself she has no real obligation, etc.). Alternatively, if feeling empathy can lead to rewarding feelings from helping, she might cultivate empathy in cases where cooperation will benefit her. This self-regulation of emotion highlights an interesting facet: IU could endorse *modifying one’s own preferences or feelings* if that ultimately increases happiness. In other words, the agent might “train” herself to enjoy doing things that are good for her long-term welfare (a kind of enlightened self-management). This resonates with ancient eudaimonistic philosophies like Stoicism or Epicureanism, which also advised molding one’s desires to achieve tranquility (Epictetus, ~100 CE; Epicurus, ~300 BCE).

### Formalizing IU: A Rational Choice Model

We can formalize the Individual Utilitarian agent as a decision-theoretic model. Consider the agent at time $t$ with a set of possible actions $A_t = \{a_1, a_2, ..., a_n\}$. The agent has a **utility function** $U: Outcomes \to \mathbb{R}$ that assigns a real-valued utility to each possible outcome, representing the agent’s personal degree of satisfaction with that outcome. Because outcomes may be uncertain, the agent also considers probabilities. If action $a_i$ can lead to various outcomes $o_{i1}, o_{i2}, ...$ with probabilities $P(o_{ij}|a_i)$, then the *expected utility* of action $a_i$ is:

$$
EU(a_i) = \sum_j P(o_{ij}|a_i) \cdot U(o_{ij}),
$$

where $U(o_{ij})$ is the utility to *the agent* of outcome $o_{ij}$. The Individual Utilitarian decision rule is: **Choose the action $a_i$ that maximizes $EU(a_i)$.** This is essentially the same as the classical **expected utility theory** of rational choice (Savage, 1954; Von Neumann & Morgenstern, 1944), but note that $U$ encapsulates only the agent’s welfare. The outcomes $o_{ij}$ can include anything happening in the world (to anyone), but the utility function evaluates them purely from the agent’s perspective.

For example, let’s formalize a simple scenario: the agent can either *lie* or *tell the truth* in a situation. Lying might bring a financial reward but also risk guilt or reputational harm if caught. Telling the truth might forgo the reward but keep conscience clear. The agent will weigh:
- $EU(\text{lie}) = P(\text{not caught}) \cdot U(\text{money gain but feel guilty}) + P(\text{caught}) \cdot U(\text{money + guilt + punishment})$.
- $EU(\text{truth}) = U(\text{no money but self-respect intact})$ (assuming no uncertainty in outcome of truth).

If the agent’s **U** heavily penalizes guilt and values self-image, it might turn out that $EU(\text{truth}) > EU(\text{lie})$, so she tells the truth (for self-interest reasons). If instead the agent doesn’t feel much guilt or thinks getting caught is unlikely, $EU(\text{lie})$ could be higher, and she lies. In both cases, the decision appears “moral” or “immoral” to an external observer, but to the IU agent it is simply rational calculation about her well-being. 

This rational choice formulation shows that IU is, in a sense, *not a distinct mathematical framework from standard decision theory* – the key difference is in the content of the utility function and what counts as “utility.” In classical utilitarianism, one could imagine a benevolent decision-maker with a utility function that sums over everyone’s happiness. In IU, the utility function picks out just one person’s happiness. In effect, classical utilitarianism can be seen as multi-person or *universal* expected utility maximization, whereas IU is *single-agent* expected utility maximization. Henry Sidgwick in *The Methods of Ethics* recognized these as two competing “Methods” – *rational benevolence* (utilitarianism) and *rational prudence* (egoism) – and famously noted the **“dualism of practical reason”** that arises because pure reason doesn’t obviously tell us why one should count others’ good equal to one’s own (Sidgwick, 1874). IU, as a theory, straightforwardly chooses rational prudence (egoism) over benevolence as the fundamental way to frame ethics.

Before proceeding to comparisons, it’s worth emphasizing the **scope** of IU: It is a theory addressed to *individual moral cognition* – how should an individual make choices? It does not directly address what social policies or rules are best (except indirectly: presumably a world in which each individual rationally pursues their own good could still have rules-of-thumb or social contracts that help each person avoid destructive conflict, as we’ll discuss). IU also does not inherently condemn or praise any particular outward action as universally right or wrong; everything depends on the specific agent’s situation and feelings. For one person, donating to charity might be utility-maximizing (maybe they are empathetic and wealthy); for another, it might not be. IU yields a *relative* guidance: each person should do what is best for themselves, given their own constitution and circumstances. 

In the subsequent sections, we will see how this model stands up against other ethical frameworks and what consequences it entails. But having delineated IU’s principles and process, we can proceed with a clearer picture of what we are evaluating.

## Comparison with Classical Utilitarianism

At first glance, *Individual Utilitarianism* may appear to be a contradiction in terms: utilitarianism is usually associated with impartial concern for all, whereas IU is explicitly partial to one individual. To better understand IU’s philosophical identity, we must contrast it with the classical utilitarian doctrine from which it conceptually descends. Here we compare the two on key aspects: scope of moral concern, the role of impartiality, and implications for moral choice.

**Scope of Concern:** Classical utilitarianism (as developed by Bentham, Mill, and Sidgwick, among others) holds that *every individual’s* happiness or utility is part of the total good that morality should maximize (Mill, 1863; Sidgwick, 1907). The moral agent is supposed to treat her own happiness as just one element in a grand sum of utilities. In utilitarian terms, if an agent is choosing an action, she ought to consider *all* people (perhaps even all sentient beings) who will be affected, summing up increases and decreases in well-being across persons. Bentham (1789) famously introduced the idea of a “hedonic calculus” where each person’s pleasure and pain are inputs, with no person’s welfare counted as inherently more important than another’s. By contrast, Individual Utilitarianism restricts the scope to a singular locus: *the agent’s own well-being.* Others’ pleasures and pains are *excluded* unless they causally influence the agent. Thus, IU can be seen as utilitarianism with a *boundary condition*: only one person’s utility function matters.

One might say that classical utilitarianism is **universalist** and **aggregative**, whereas IU is **singular** and **non-aggregative**. The classical view, for instance, sees moral dilemmas like whether to save one person or five people as straightforward (other things equal, saving five is better because five experiences of well-being outweigh one). IU would see it differently: if it’s *my* choice and *my* life isn’t at stake, the only reason I might save the five over one is if I personally care about the number (maybe I’d feel better about saving more lives, or avoid guilt/public scorn). But if, say, the one person is my child and the five are strangers, a classical utilitarian says I should save the five (their combined utility loss from death is presumably bigger), whereas IU says I should save my child if that’s what maximizes *my* well-being (which likely it does, as losing my child would devastate me, a far worse outcome for my happiness than the alternative). In this way, IU aligns with common intuitions of partiality (we naturally care more for family or friends), but it makes those *the* moral truth rather than a permissible exception.

**Impartiality vs. Partiality:** A hallmark of classical utilitarian ethics is its impartial stance: *“Everyone to count for one, no one for more than one.”* This impartiality is considered by utilitarians both a strength (morality requires impartiality) and a challenge (it demands a high level of selflessness). John Stuart Mill (1863) emphasized that one person’s happiness is *just as valuable* as another’s in the moral calculus; the moral viewpoint is often described as the “point of view of the universe” (Sidgwick, 1907) – a phrase indicating that from the moral standpoint, we abstract away from our personal perspective and weigh all interests equally. IU pointedly *rejects* this impartiality. It says essentially: *“From my own perspective, it is rational and right that I treat myself as ‘counting for one’ **and others as counting for zero, except insofar as they enter my utility calculations**.”* This is a maximal form of **agent-relativity**. In moral philosophy terms, classical utilitarianism is *agent-neutral*: the fundamental reason to promote utility applies equally to anyone’s utility (Nagel, 1986). IU is *agent-relative*: the fundamental reason to promote utility applies only to the agent’s own utility. 

It’s interesting to note that classical utilitarianism, despite its impartial aspirations, can be *interpreted* in an agent-relative way by an egoistic agent under certain conditions. For example, a utilitarian society might enforce rules that make it in everyone’s self-interest to behave impartially (via social sanctions or rewards). But the pure ethical injunction is impartial regardless of personal interest. IU by definition has no such injunction; it explicitly elevates partiality toward self as the correct approach.

**Moral Symmetry and Asymmetry:** Bentham and Mill both argued for the moral equality of persons: each person’s happiness is equally important. This leads to a kind of symmetry – if you trade places with someone, the ethical evaluation of a scenario should remain the same (since just who feels the pleasure/pain changes, but total stays same). IU introduces a deep **asymmetry**: the identity of the agent is morally crucial, because only that person’s feelings matter. If the IU agent imagines trading places, the evaluation *completely flips*. For instance, imagine two individuals, Alice and Bob, with conflicting interests. From a social-utilitarian perspective, one might seek a compromise or weigh their utilities to decide what’s best overall. From Alice’s IU perspective, only Alice’s utility counts, so whatever benefits Bob is irrelevant except via Alice. From Bob’s IU perspective, only Bob’s utility counts, leading potentially to a stalemate or conflict (this will be discussed further in the section on ethical egoism and conflict).

**Decision Procedure Differences:** In practice, a classical utilitarian trying to decide “What should I do?” must, at least in principle, consider how each possible action affects everyone. This can be computationally and epistemically daunting, which is why utilitarians often rely on rules or heuristics (rule utilitarianism, or common-sense morality as a guide). An IU agent’s deliberation is in principle simpler: she considers only her own utility. This might seem easier, though in reality people’s utility functions are entangled with others. But at least she’s not morally obliged to agonize over everyone’s interests, only her own. This narrower focus might allow more clarity in some decisions (no need to perform a global calculation each time).

**Example Comparison – Charity Dilemma:** Reprising the earlier example of charitable donation: A classical utilitarian would weigh the loss of utility to herself by giving money against the gain of utility to the beneficiaries. Because typically a sum of money provides more utility to a very poor person than the same sum’s loss of utility to a comfortably well-off donor (due to diminishing marginal utility of money), classical utilitarianism often says large charitable donations are morally required (Singer, 1972). From IU’s perspective, the donor’s loss vs. gain to others matters only via her own feelings. If she’s empathetic or has a moral identity tied to altruism, she might still donate a lot (because seeing suffering and knowing she can help pains her). But if she’s not particularly moved, IU gives her no further reason to donate. In fact, IU would condone her spending the money on personal luxuries if that makes her happier. This highlights a fundamental divergence: **classical utilitarianism tends to demand altruism**, while IU **permits or even demands egoism**. 

To a classical utilitarian critic, IU misses the essential moral insight that *others matter.* To an IU advocate, classical utilitarianism demands an unrealistic (and perhaps unjustified) self-sacrifice: Why *should* the agent treat others’ happiness as equal to her own? IU aligns with the intuitive idea that each person naturally cares for themselves first and that perhaps reason doesn’t compel us to abandon that standpoint completely (this is similar to Bernard Williams’s critique of utilitarian “alienation” – that demanding we treat even our own projects and relations impartially can alienate us from our identities and commitments (Williams, 1973)). IU simply makes that personal standpoint the basis of morality.

**Bridging the Gap?** It is worth noting that some thinkers have explored whether the chasm between prudence and ethics (egoism vs utilitarianism) can be bridged. One argument in the literature is that in the long run, it’s in each individual’s self-interest to adopt generally benevolent dispositions because of social consequences (this is sometimes called “enlightened self-interest”). For example, Henry Sidgwick acknowledged that *“enlightened self-interest would direct most men to foster and develop their sympathetic susceptibilities”*. If one internalizes compassion, one will get along better with others and likely be happier (since humans are social). Classical utilitarians might say: “If everyone pursued their own greatest happiness *with enlightenment*, they would end up behaving in ways that also promote general happiness.” In an ideal scenario, perhaps everyone’s IU leads them to cooperative behavior that looks utilitarian. This is essentially the idea behind **invisible hand** arguments or contractarian approaches (Gauthier, 1986): self-interest can yield socially optimal outcomes under certain conditions (as in repeated Prisoner’s Dilemmas where cooperation is the best long-term strategy for each). 

However, the difference remains that IU only values the social optimum as a means to personal optimum, whereas classical utilitarianism values it inherently. As a result, there are cases where the two prescribe different actions. For instance, consider a scenario: A doctor can save five patients by killing one healthy patient and distributing his organs (a typical utilitarian dilemma). A classical act-utilitarian might, controversially, say this is morally permissible or even required if it really maximizes total lives saved (assuming no other consequences). But an IU-minded doctor would consider: “If I do that, I’ll probably be wracked by guilt, possibly caught and punished, and I’d violate my personal integrity as a healer. All those are bad for me.” So likely IU says “don’t kill the patient,” aligning with common moral intuition. In this scenario, the impartial utilitarian perspective clashes with individual-focused perspective; interestingly, IU might coincide with common sense because our common moral intuitions often have a strong self-regarding element (we’d feel horrible murdering someone).

**Summary of Comparison:** Classical utilitarianism and Individual Utilitarianism share a *family resemblance* in that both use a consequentialist, happiness-based logic. But they diverge on the crucial dimension of whose happiness counts. Classical utilitarianism asks individuals to be **impartial spectators** maximizing the sum-total of welfare; Individual Utilitarianism asks individuals to be **maximizers of their own welfare**. This essentially turns morality inward. It’s as if the scope of moral community in IU shrinks to one – a community of one individual across time. Others become part of the environment rather than part of the moral equation except insofar as they affect that one. This reorientation answers one longstanding question (“Why care about others?”) with, essentially, “You don’t have to, except when it affects you.” It is a stark answer that many ethicists from Hume to Kant to Singer would reject as missing the point of morality. Yet, it captures an important perspective in ethical theory akin to **ethical egoism**. In fact, to fully understand IU, we must compare it to ethical egoism and see if there’s any daylight between them – which we will do after first addressing deontological ethics, a framework nearly antithetical to IU’s approach.

## Comparison with Deontological Ethics (Kantian Perspective)

If classical utilitarianism is a near-cousin turned away by IU’s individualism, **Kantian deontology** is almost an opposite from the get-go. Kantian ethics is built on the idea of absolute moral duties and respect for persons as ends in themselves (Kant, 1785). Individual Utilitarianism, by contrast, is consequentialist, calculating, and treats persons (except the self) as means. This section contrasts IU with deontological principles, focusing on Kant’s moral philosophy as a prime example of deontology.

**Ends and Means:** One of the most famous formulations of Kant’s Categorical Imperative is the **Formula of Humanity**: *“Act so that you treat humanity, whether in your own person or in the person of any other, always at the same time as an end, never merely as a means”* (Kant, 1785, **Groundwork of the Metaphysics of Morals**, II). This principle demands that we recognize the intrinsic worth of each rational being. To treat someone as a mere means is to use them for your own purposes without regard to their own goals and dignity. This is morally forbidden in Kantian ethics. Individual Utilitarianism, however, programmatically treats others as means to the agent’s happiness. In IU, failing to consider others’ own ends is not a moral failing; it’s practically built into the theory as long as ignoring their ends doesn’t backfire on your ends. For a Kantian, this is a profound moral error: IU’s stance would be seen as the core of *immorality*, since it disregards the moral law that persons must be respected.

Consider a concrete case: lying. Kant famously argued that lying is always wrong because it treats the person lied to as a mere means (to your convenience or advantage) and undermines the principle of honest communication necessary for respect and trust (Kant, 1797). An IU agent, in contrast, will lie whenever lying is better for her personal utility (unless the disutility from guilt or being caught outweighs the benefit, as earlier analysis showed). So if she can gain from lying with little personal cost, IU says she should lie; Kant says she must not, even if no immediate harm seems to come from it, because it violates a duty to truthfulness and respect.

**Moral Duty vs. Utility Calculation:** Deontological ethics often emphasize doing one’s duty for its own sake, not for the consequences. The Kantian moral agent is motivated by respect for the moral law—a sense of obligation that overrides self-interest. In fact, Kant drew a sharp distinction between **inclination** (acting from desires, including desire for happiness) and **duty** (acting from moral principles). The *only* thing that makes an action truly morally worthy, in Kant’s view, is doing it from duty, not just in accord with duty (Kant, 1785). If a person helps others out of compassion or because it makes them feel good (i.e., out of inclination), Kant would say that has no moral worth, even if the action is right; it’s basically a fortunate coincidence of self-interest and duty. The truly moral person would help even if they had no inclination, purely because it’s the right thing. 

Individual Utilitarianism, on the flip side, collapses duty into inclination (broadly understood). It tells the agent to *follow her enlightened inclination*, i.e., her calculated self-interest. In IU, there is no concept of “duty” that might override what you want; what you ought to do *is precisely* what best fulfills your wants (long-term). This means IU cannot accommodate the idea of *moral constraints* that trump expediency. Deontologists like Kant think some actions are wrong no matter how much utility (for oneself or others) they produce—e.g., murdering an innocent person is wrong even if you somehow benefited greatly, because it violates a categorical duty. IU has no absolute prohibitions; any action could be justified if it maximizes the agent’s utility. This is a classic feature of **consequentialism** vs **deontology**. IU, as a form of consequentialism, cares only about outcomes (for the self), whereas deontology cares about the nature of actions and adherence to rules or rights.

**Universality vs. Personal Point of View:** Kant’s ethics is strongly **universalist** in a different way than utilitarianism. The Categorical Imperative in its universal law formulation says: act only according to maxims that you could will as a universal law (Kant, 1785). This demands an agent consider whether the principle of her action could be consistently adopted by everyone. It’s a test that inherently asks you to step out of your personal perspective and consider the form of your action in general. IU’s perspective is inherently first-personal and not universalizable in Kant’s sense. If an IU agent tries to universalize her maxim, it might be something like, “Everyone should maximize their own happiness.” Paradoxically, that is universal, but it doesn’t pass Kant’s test because if everyone is just pursuing their own ends, we might get contradictions when these ends conflict. More directly, Kant would likely say IU’s guiding maxim fails to treat humanity as an end and thus couldn’t be willed in a moral community.

It’s also notable that Kantian ethics would view the *motivation* of an IU agent as non-moral. For Kant, acting to maximize your happiness is a natural motive, but it has no moral worth unless it aligns with duty—and if it conflicts with duty, it is a temptation to be resisted. IU elevates what Kant sees as non-moral (self-interest) to the status of the sole moral principle. 

**Rights and Constraints:** Beyond Kant, deontological theories generally hold that individuals have rights that place constraints on what we can do to them. For instance, contemporary deontologists would say individuals have a right not to be harmed or coerced unjustly, even if harming one might benefit another. IU does not inherently recognize rights—except to the extent the IU agent *cares about* respecting rights (maybe to avoid retaliation or because violating someone’s rights causes the agent discomfort). If an IU agent can infringe someone’s rights with impunity and benefit from it, IU has no internal principle to forbid that. Deontology’s emphasis on rights and constraints is about acknowledging others as separate centers of value who cannot be simply traded off against personal gain. IU recognizes others as separate, but says “only *my* center of value is to be maximized.” Thus, from a rights perspective, an IU agent is not bound to uphold others’ rights except instrumentally. This would be deeply problematic in a deontological framework, where respecting rights is a moral duty not contingent on personal gain.

**Moral Relationships:** Another angle is how each framework views relationships and promises. In Kantian (and other deontic) ethics, making a promise creates an obligation to keep it. The obligation isn’t just based on outcome; it arises from the act of promise itself and respect for the trust given by the promisee. An IU agent will keep promises only so long as it benefits her (or failure to keep them harms her). So, if circumstances change and breaking a promise would advantage the IU agent with minimal fallout, IU says break it. Kantians would say that violates the duty of fidelity. Similarly, Kant’s ethics valorizes certain virtues like honesty, fidelity, beneficence (with the nuance that beneficence is a duty to help others, though a *meritorious* duty that allows latitude in how much one helps). IU would only pursue those virtues if they coincide with self-interest.

**Exception: Duties to Oneself:** One small overlapping point: Kantian ethics does recognize duties to oneself (like not destroying oneself, not debilitating one’s rational capacities, etc.). IU obviously aligns with many duties to oneself, since IU wants the agent’s well-being. For example, Kant says suicide is immoral because it treats one’s own life merely as a means to escaping suffering (violating self-respect and the rational nature given by God or nature). IU would say suicide is wrong *for the agent* if living would overall contain more happiness than pain or if the reasons for suicide are transient. However, IU would actually allow suicide if a person’s future prospects are predominantly suffering (because then maximizing lifetime well-being might mean curtailing extreme prolonged pain). Kant forbids suicide regardless of utility calculation, because life has a duty of preservation. So even here, the rationales differ: IU is about personal utility calculus; Kant is about a moral law about respect for humanity (including one’s own).

**Evaluation:** From a Kantian deontologist perspective, Individual Utilitarianism would likely be seen as a form of **immoralism** – a codified selfishness that ignores the moral law. It exemplifies treating persons (other than oneself) as mere means and elevating inclination (desire for happiness) above duty. It fails the categorical imperative test and cannot be a basis for a moral society, because a society of pure self-maximizers lacks the mutual respect and trust under law that Kantian ethics sees as essential. Indeed, one might say IU is anathema to Kantian ideals of a *kingdom of ends* (a community where each treats themselves and others as ends and obeys moral laws they give themselves rationally). IU’s community of one falls short of any such moral community.

Conversely, from an IU viewpoint, Kantian deontology might appear overly restrictive and divorced from human motivation. IU might question: if moral duties require going against one’s own happiness entirely (as in some extreme cases), why should an individual adhere to them? What justifies these absolute commands from reason? The IU theorist could argue that Kantian ethics demands a kind of self-sacrifice to abstract principles that is ultimately irrational from the individual’s standpoint, expecting a purity of motive (duty for duty’s sake) that few humans attain and that, arguably, doesn’t lead to happiness. Kant famously said he doesn’t ground morality in the pursuit of happiness at all; for IU this is a non-starter since happiness (of the self) is the only ground. Thus they talk past each other: one is about what is rational from the perspective of moral law, the other from perspective of individual benefit.

**Middle Ground?** There are intermediary positions, like **contractualism** or **contractarianism**, which try to reconcile self-interest with moral constraints by appealing to hypothetical or actual agreements. A contractarian (à la Hobbes or Gauthier) might say that rational self-interest leads agents to agree on certain moral rules (like not harming each other, keeping promises) because that ultimately serves everyone’s interests by avoiding the state of nature’s chaos. This is a possible bridge: it effectively says that if you follow IU in the long run, you’ll see the wisdom of certain deontological-looking rules as conventions that make everyone better off. However, those aren’t categorical in principle; they are conditional on continuing to serve self-interest. Kant would reject that approach because for him morality isn’t a contingent deal, it’s an unconditional requirement of reason. But it does highlight that an IU society could *function* if everyone is rational and foresighted: they’d act nicely to each other to gain mutual benefits (kind of a *modus vivendi* morality). Yet, at the level of principle, IU remains fundamentally different: no action is forbidden if it truly maximizes self-interest and one can get away with it, whereas deontology forbids some actions even then.

In summary, Individual Utilitarianism and Kantian deontology represent two poles:
- IU: All about **consequences for me**; no absolute rules beyond what maximizes my good.
- Kantian ethics: All about **absolute moral rules** and respect; consequences are irrelevant to the rightness, and personal benefit is morally irrelevant.

This stark contrast shows the range of ethical theories: from the most relative and self-centered to the most universal and duty-bound. Each side would levy serious criticisms at the other. The IU proponent might see Kantian morality as noble but impractical or unmotivating (“Why be moral if it doesn’t benefit me?”). The Kantian sees IU as essentially amoral (“Just selfishness with a calculator”). Each highlights genuine tensions in ethical theory: reason vs desire, self vs others, duty vs happiness.

With the extremes of utilitarian impartiality and Kantian duty considered, we now turn to examine IU against the theory it most closely resembles: **ethical egoism**. Given that IU essentially says one should act in one’s self-interest, is there any meaningful difference between IU and egoism? Does IU contribute anything beyond what a refined ethical egoism already states? Let’s explore that next.

## Comparison with Ethical Egoism and Rational Egoism

Individual Utilitarianism bears a *strong resemblance to ethical egoism*, the view that each person ought to pursue their own self-interest exclusively (Regis, 1980). In fact, as noted earlier, some authors have used terms like “individual utilitarianism” interchangeably with *“egoistic hedonism”*. Therefore, it is crucial to clarify what (if anything) distinguishes IU from standard ethical egoism and to see how IU addresses common critiques of egoism. Additionally, we will compare IU with the notion of **rational egoism** (the idea that it is rational to act in one’s self-interest) to see if IU is effectively the same as that.

**Ethical Egoism Recap:** Ethical egoism, in normative ethics, asserts that *each person morally ought to do whatever is in their own best interest.* It doesn’t necessarily say people always do act that way (that would be psychological egoism, a descriptive claim), but that they should. It’s a consequentialist theory in a sense: it cares about consequences to the agent’s own welfare. Ethical egoism can come in a strong form (always act in your own interest, and it’s never moral to not do so) or a weak form (it’s always moral to pursue your interest, but there might be rare cases where not doing so isn’t immoral). IU clearly falls in line with strong ethical egoism: by definition, IU says you should always maximize your own utility and it’s never morally right to knowingly do something that leaves you worse off for the sake of others.

Given this alignment, one might ask: why not just call IU “ethical egoism”? The answer may lie in IU’s particular emphasis on *utilitarian structure and subjective utility calculus.* IU is egoism cast in the language of utilitarianism and decision theory. Traditional ethical egoism might be stated in commonsense terms (“look out for number one” or “everyone’s only moral duty is to themselves”). IU dresses this in a more analytic framework: talk of “utility”, “maximization”, “cumulative well-being”, etc., drawing from utilitarian and economic terminology. This might make IU a more specific or sophisticated version of egoism, but substantively, they agree on the basic normative directive.

**Rational Egoism:** This is the idea that it is *rational* to act in one’s self-interest and irrational to act against it. Ayn Rand, for instance, championed a form of rational egoism where pursuing one’s own life and happiness is the essence of rational morality (Rand, 1964). She termed it the “virtue of selfishness,” arguing one should never sacrifice oneself to others nor others to oneself, but rather trade value for value to mutual benefit. IU might diverge from Rand slightly: Rand also believed in respecting others’ rights and rational nature (so she was against fraud or coercion even if you could benefit, because she had a deeper philosophic commitment to individual rights and reason). IU, as we outlined, doesn’t have that rights constraint unless it circles back to self-interest. But both IU and Randian egoism share the view that *it’s irrational not to pursue your own interest.* In fact, Rand accused altruistic moralities of being inherently irrational or contrary to life’s requirements.

Henry Sidgwick discussed rational egoism as a principle co-equal with utilitarianism in terms of self-evidence (Sidgwick, 1907). He found no decisive refutation of the proposition that one’s own good is the ultimate rational aim for each individual (that’s what led to his dualism of practical reason). IU essentially takes that proposition and runs with it as the sole foundation, whereas Sidgwick personally *hoped* to reconcile it with universal benevolence (though he admitted difficulty).

So, IU certainly embodies rational egoism: it claims that not only is it moral, but it’s the rational thing to do, to maximize your own good. If an IU agent knowingly acted against her own interest (say, gave up a major benefit for herself with no expectation of return just to help strangers, and it made her unhappy), IU would deem that irrational and morally mistaken (in fact, under IU those coincide).

**Subtle Differences (If Any):** Possibly IU’s unique addition to egoism is the *utilitarian flavor* – it’s concerned with *maximizing aggregate happiness over one’s life.* Traditional egoism might not explicitly talk about aggregating utility over time, though obviously a prudent egoist cares about long-term welfare. IU’s language of cumulative well-being and calculation could be seen as making explicit that the egoist’s task is akin to the utilitarian’s, but with a singleton population (oneself). It also implies a kind of personal *impartiality across time*: you treat “future you” and “present you” without arbitrary bias, planning for the best overall life outcome (this is sometimes called *temporal neutrality* or prudence). Ethical egoists often emphasize long-term self-interest to avoid the misconception that egoism = short-term greed. IU strongly concurs: a true IU agent considers future consequences carefully (e.g., that lying now might undermine trust and hurt her later, etc.). So IU has that emphasis built in.

Another nuance: IU allows that others’ welfare *may* indirectly matter, via the agent’s utility function (we discussed empathy, love, etc.). Some caricatures of egoism ignore these subtle ways that caring for others can be part of one’s interests. IU doesn’t ignore that; it actually can accommodate genuine affection and even self-sacrifice for loved ones *if* the agent’s happiness is bound up in theirs. But ethical egoism literature has noted the same: egoism doesn’t say you must be selfish in a narrow sense, it’s okay to help others if it’s in your interest (and often it is, because of emotional connections and social reciprocity). The difference is just that IU formalizes that via subjective utility.

**Critiques of Egoism:** Ethical egoism has been widely criticized in philosophy. Let’s see how IU might respond differently (or similarly) to these critiques:

1. **Arbitrariness and Public Justification:** Perhaps the most common objection (raised by thinkers like James Rachels) is that egoism arbitrarily elevates one person’s interest (the agent’s) over everyone else’s, without justification. Morality, critics say, requires a kind of impartiality or at least a reason why one person should matter more. If I as an egoist say “my interests matter more because they’re mine,” that sounds circular or unjustified. Rachels argued that there is no relevant difference between oneself and others that justifies exclusive concern, hence egoism is unacceptably arbitrary (Rachels, 2003). IU doesn’t offer a new rebuttal beyond the standard egoist one: which might be, “From my perspective, it’s not arbitrary at all—I *am* the one experiencing my life, so it’s natural to care about that. Morality is not an abstract view from nowhere, it starts from individual perspectives.” Egoists sometimes appeal to intuition: no one has a better reason to look after you than you do. But Rachels’ point is ethical principles should extend universally unless there’s a difference. IU’s stance is basically affirming the agent-relative difference.

   The arbitrariness objection also ties to the **inability to resolve conflicts**: If everyone followed egoism, when interests conflict, there is no higher principle to appeal to. Each says “I matter.” In moral disputes, we typically look for a fair solution; egoism doesn’t provide one beyond negotiation or force. IU as a theory doesn’t solve conflicts except by suggesting mutually beneficial compromise when possible. If two IU agents conflict in interests, they may either fight or find a bargain. Moral philosophy usually hopes for a principled resolution; egoism offers only strategic resolution (like Nash bargaining or something). Critics see that as a failure of egoism to be a moral theory, because morality should guide conflict resolution, not just describe power struggles (Barry & Hospers, 1967).

   IU’s possible defense: In practice, if everyone is rational, they might avoid costly conflict (akin to how in a market, self-interest leads to trades, not just constant theft, because theft begets retaliation and insecurity). This sounds more like game theory than ethics – indeed, egoism often reduces morality to game-theoretic equilibrium. IU as a normative view might say, “It’s not my job to resolve others’ conflicts, only mine. If others also follow IU, each will stick up for themselves; hopefully we can find arrangements that make each of us as well-off as possible (Pareto optimal outcomes). If not, that’s unfortunate, but there’s no overarching moral order guaranteed.” This is a very non-ideal, perhaps bleak, view compared to moral theories that promise justice for all. It’s essentially a form of **moral relativism to persons**—each person’s good is the supreme law for them, and there is no overarching moral law binding them together unless they create one by agreement.

2. **Insoluble Conflict Leading to “Might makes right”:** Some critics argue egoism would effectively endorse a might-makes-right world. If two egoists want the same resource, the stronger will take it, and egoism has nothing to say against it (indeed, the stronger is just pursuing his interest successfully). This seems morally unsatisfactory. Proponents like Rand or other rational egoists counter that *rational* egoism would see fighting as costly and would prefer peaceful trade or division of resources. The IEP article on egoism describes that an egoist can argue that cooperation is in each person’s interest up to a point. IU can incorporate that: often, working out a fair share is better than a violent clash where you risk injury. So egoism doesn’t always devolve into brute force; enlightened egoists form communities and respect certain rules because it’s mutually advantageous (Hobbes’s social contract is grounded in this idea).

   However, if the power imbalance is big, might may well win. Egoism doesn’t morally condemn that beyond saying the loser’s perspective finds it bad but the winner’s perspective finds it good. This is uncomfortable because it means no moral protection for the weak. Most moral theories try to protect the weak (utilitarianism by counting them in the sum, deontology by rights, etc.). IU simply does not provide that assurance. It is very much a *victor’s morality* in conflicts—except that the “victor” and “loser” are moral only relative to themselves. This critique stands and is often considered a fatal flaw if one believes morality should be something that solves prisoner’s dilemmas by advocating cooperation even when it’s risky, etc.

3. **Unfalsifiability / triviality:** We will elaborate more in the critique section, but psychological egoism – the claim everyone is motivated by self-interest – is often dismissed as unfalsifiable because any action can be reinterpreted as selfish (e.g., someone says “I want to help others”; psychological egoist: “you only *want* to because it fulfills *your* desire, so it’s selfish”). Ethical egoism doesn’t have the exact same issue since it’s normative, but it can seem trivial if every action an agent does can be described as what they wanted to do in that moment. This is a classic defense by egoists: “whenever you act, you’re doing what at that moment you most want or choose, so you’re always pursuing your interest by definition.” This definition of “interest” becomes so broad it lacks content. IU avoids that by focusing on *well-being* rather than momentary desire. It acknowledges you might have short-term impulses that aren’t actually in your best interest. So IU might differentiate between *actual utility* and *perceived utility*. If someone does an altruistic act feeling obligated, they might not enjoy it, so maybe they didn’t maximize their utility; IU would say they acted against what they should have done if that’s the case. But what if helping gave them a deep sense of fulfillment? Then they did maximize their utility even while seeming “selfless.” This is why egoism and IU often argue much apparent altruism is perfectly consistent with egoism, since it rewards the helper in some way (psychologically, etc.). Critics might call that a self-sealing argument.

   IU’s response might be: We can conceive of cases where someone truly gets nothing (not even emotional satisfaction) from a dutiful act, then egoism predicts they shouldn’t do it, which conflicts with common moral intuition. Egoists bite the bullet: yes, they shouldn’t from our viewpoint. For example, a soldier falling on a grenade to save comrades – he dies painfully (no future benefit), in the moment he might get satisfaction of saving others but he’s dead, so arguably his lifetime utility is reduced. Egoism would say that’s not rational for him. Many would call it heroic and moral. So egoism clashes with some moral intuitions. IU would say the soldier’s action is only justified if his love or duty to comrades was so important to his values that not doing it would be worse (maybe he couldn’t live with himself knowing he let them die, thus in his mind his utility calculus favored sacrifice). If it didn’t, he wouldn’t do it. But real cases of extreme self-sacrifice are hard for egoism. They either reinterpret the utility (maybe the soldier values honor so much that dying honorably is better to him than living with cowardice – that’s a possible egoist reinterpretation), or they just say it was a mistake from his self-interest view.

4. **Egoism as a “failure to be a moral theory”:** Some contend that egoism doesn’t really count as a moral theory at all, because morality by definition is about resolving conflicts of interests and guiding behavior in a social context, not just telling each person “do whatever benefits you” (which they might do anyway). It lacks the interpersonal normative force we expect of ethics. It doesn’t tell others why they should accept or tolerate your behavior unless it benefits them; it doesn’t give grounds for blame except “you were stupid from your perspective.” J.L. Mackie (1976) noted that morality serves to regulate how we behave towards each other; egoism doesn’t regulate, it just describes each one’s pursuit.

   An IU advocate might argue that moral theories that demand self-sacrifice often face the problem of motivation (why should I follow it?). IU ensures morality is aligned with motivation (it gives you reason to follow it because it’s your own good). However, it’s true that IU doesn’t prevent someone else from harming me if it’s in their interest, which standard morality tries to morally dissuade them from. Egoism sort of says to others: “I do what’s best for me, I expect you to do what’s best for you; if those conflict, we either fight or negotiate.” That’s a kind of descriptive realism. But moral anti-realists or skeptics sometimes say maybe all moralities reduce to that when push comes to shove, so why not be honest about it?

**Conclusion of this comparison:** Individual Utilitarianism is essentially a form of **ethical egoism with a utilitarian calculus**. It shares egoism’s merits (focus on the individual, alignment with self-interest) and its demerits (accusations of arbitrariness, conflict with the moral intuition of equality, lack of conflict-resolution principle). IU’s invocation of “utility” and rigorous maximizing might make it seem more precise than a vague egoism, but substantively it does not escape the classic critiques of egoism. It stands on that same island: insisting that morality is about rational self-care, not self-sacrifice or impartiality.

However, it’s possible to argue that IU, by framing egoism in utilitarian terms, allows using some of the **analytical tools of utilitarianism** (like game theory, cost-benefit analysis, etc.) to derive more nuanced implications. For example, IU might explore under what conditions self-interest aligns with cooperation (Prisoner’s Dilemma, etc.), or how an IU agent might commit to certain principles (perhaps an IU agent might *pre-commit* to punish defectors to deter them, which looks like following a rule even at cost – something egoists discuss). In other words, IU might be an attempt to build a more *scientific egoism* as opposed to just the philosophical stance.

Now, having compared IU to these ethical theories, we shall shift perspective. Instead of comparing to another normative theory, let’s consider how IU relates to a more *existential or phenomenological* perspective: existentialism. Existentialism isn’t a single ethical theory but has implications about how individuals find value and meaning. This will let us see IU from the angle of individual meaning-making and authenticity, adding another dimension to our understanding.

## Existentialist Perspectives and Individual Utilitarianism

Existentialism, broadly speaking, is a philosophical movement that emphasizes individual freedom, choice, and the creation of meaning in a seemingly indifferent or absurd world (Sartre, 1943; Camus, 1942; de Beauvoir, 1947). While not an ethical theory in the narrow sense, existentialism offers insight into how individuals relate to their own values and to others. It often highlights the tension between *authentic self-definition* and the influence of *the “Other”* (other people and society). In this section, we will explore how Individual Utilitarianism might be viewed through an existentialist lens: Is IU an expression of radical individual freedom to define one’s own good? Or does it fall into what existentialists would call “bad faith” or a shallow mode of existence? How does IU’s treatment of others compare to existentialist analyses of intersubjectivity?

**Authenticity and Self-Created Values:** A central idea in many existentialist writings (e.g., Sartre’s and de Beauvoir’s) is that in the absence of predetermined moral values (often presupposing an atheistic or agnostic outlook), individuals must *choose their values and give their life meaning*. Jean-Paul Sartre famously said *“existence precedes essence”* – we exist first, and then through our actions define who we are (Sartre, 1946). Authenticity involves owning up to one’s freedom and responsibility in creating values, rather than just following societal expectations. On the surface, Individual Utilitarianism could be seen as a person deciding that *their own happiness is the ultimate value* and consciously structuring their choices around that. This could be interpreted as a kind of authenticity: the person is not simply following some external moral code imposed by religion or society (like utilitarian concern for all, or Kantian duty, etc.), but rather asserting, “The meaning of my actions is to make *my* life as good as possible for me.” In effect, they choose self-utility as their guiding value.

One could liken this to **Nietzsche’s** idea of the *Übermensch* who creates values anew in a godless world, though Nietzsche’s vision is more about creating life-affirming values beyond mere pleasure. Nietzsche criticized the “herd morality” (which values altruism and equality) and might see IU as at least rejecting herd morality’s demand to prioritize others. However, Nietzsche also valorized *power, creative risk, and overcoming* rather than comfortable happiness. So an IU agent who just seeks comfortable happiness might appear *Philistine* to Nietzsche – too bourgeois and not aspirational enough. Existentialists like Sartre and Camus often valued authenticity and engagement with life’s challenges over simple happiness.

Sartre might say: if pursuing your own happiness is the project you freely choose, then that is your self-created essence. But existentialism would challenge: *Is that project chosen authentically, or is it a flight from the heavier burdens of freedom?* There’s a concept of **“bad faith”** in Sartre’s philosophy: it’s when one deceives oneself to escape the anguish of absolute freedom (Sartre, 1943). If someone in IU mode says “I will just maximize my comfort and pleasure,” one could argue this might be avoiding deeper questions of meaning or moral responsibility, which could be labeled as a form of bad faith if it’s an escape from considering the value of others or higher ideals. That said, authenticity doesn’t require altruism—one could authentically choose an egoistic life path, but existentialist thinkers often emphasize that ignoring the reality of others’ freedom or denying connections can lead to conflict and self-deception (Sartre’s analysis of relationships in *Being and Nothingness* found many are tainted by attempts to deny the other’s subjectivity or one’s own, e.g., sadism or masochism in interpersonal dynamics).

**The Other and Interpersonal Dynamics:** Sartre’s *Being and Nothingness* contains an examination of relations with others that is famously pessimistic: “Hell is other people” (Sartre, 1944) is a well-known quote (from his play *No Exit*), capturing the idea that the presence of others can threaten one’s own freedom by objectifying one (e.g., feeling shame under another’s gaze). Sartre describes how when we become aware of the Other, we can either try to assert our freedom by making the Other an object (treat them as a thing – which is reminiscent of IU’s stance of instrumentalizing others), or we can be made an object by their gaze (feeling judged, losing our autonomy in our own eyes). This often devolves into conflict: a struggle of each consciousness to assert itself. In that sense, Sartre’s view of unguided interpersonal relations (without an ethic of mutual recognition) is not far from a Hobbesian egoist struggle or at least a permanent instability. *Ultimately, Sartre called these dynamics a form of “inter-subjective bad faith”*, because each tries to deny the full subjectivity of the other or themselves. 

If we map this to IU, an IU agent *does* in a way treat the Other as an object (a source of utility or disutility). That is exactly one of the stances Sartre analyzes: making the other an object to affirm one’s own subject (the “master” attitude in a master-slave dynamic, or the lover who wants to possess the beloved completely). Sartre would likely say this is an unstable and conflict-ridden stance; the other might rebel or withdraw, and then the agent doesn’t get what they want either. Love, in Sartre’s analysis, often turns into a struggle because each wants to be loved as subject while making the other love them as object – a contradiction. If an IU lover only cares because loving the other makes *them* happy, the other might feel that and sense they are not truly respected as independent – which could poison the relationship, ironically reducing the IU agent’s happiness as well.

**Existentialist Ethics:** It’s debated whether existentialism offers a full ethical system. Sartre at one point said we must will the freedom of others as a condition of our own freedom (Sartre, 1946), which some interpret as a thin kind of ethical principle: since I recognize my freedom as ultimately valuable, I must value others’ freedom too for consistency (somewhat like a Kantian echo, but derived differently). Simone de Beauvoir, in *The Ethics of Ambiguity* (1947), tried to develop an existentialist ethics where respecting others’ freedom is a value because our own freedom is intertwined with others’ (we exist in a world with others). According to Beauvoir, *“To will oneself free is also to will others free.”* She criticized pursuing one’s aims at the expense of oppressing others, because that actually undermines the meaningfulness of one’s own existence in a shared world (Beauvoir, 1947). If we apply that critique to IU, we’d say: a pure IU agent might, in pursuing only her own utility, limit or harm others’ freedom or well-being. From Beauvoir’s existentialist ethic perspective, that is self-defeating in a philosophical sense: you’re denying the “ambiguous” condition that you are both a subject and an object in a world of others likewise. We rely on others to recognize us, to share projects, etc. If you treat them solely instrumentally, you reduce the possibility of genuine reciprocity or mutual recognition that could confer deeper meaning on your pursuits.

**Meaning and Happiness:** Existentialists often distinguished between mere contentment and authentic fulfillment. Camus wrote of the absurd condition and suggested rebellion – living fully without hope of eternal reward – as a way to create meaning (Camus, 1942). If an IU agent says “the meaning of life is to maximize my happiness,” Camus might respond: this is a naïve substitution of quantity of pleasure for meaning, and in the face of absurdity, piling up pleasure might not suffice to feel one’s life is meaningful. People often seek meaning beyond themselves – through art, causes, love, etc. IU would interpret those as means to personal happiness, but existentialists might say reducing them to that misses their meaning. For example, an artist might suffer for their art, not because it makes them happy in the moment but because it gives their life a meaning they chose. If IU says “don’t suffer, maximize happiness,” an existentialist might say that’s a shallow prescription if the person’s authentic project involves struggle or self-transcendence.

However, note that IU doesn’t necessarily equate utility with hedonistic pleasure; it could include achievement, self-actualization, etc., if those are what the person values. So an IU agent could very well say “writing this novel is grueling but it will bring me deep satisfaction to have created something meaningful in my life, so that’s part of my utility.” In that sense, IU can incorporate some of the striving for meaning as just part of what makes the individual satisfied. The difference is existentialism might say: be prepared that your authentic project might demand sacrifice and might not maximize comfort or even subjective well-being in a simplistic sense. Existential “happiness” is not about pleasure, it’s about living in accordance with one’s chosen values under freedom. If an IU agent is broad-minded, they can align with that to a degree (their utility could come from living authentically). But if they equate utility with ease or safety, they might avoid the very challenges that give life depth (which existentialists would warn against as living in bad faith or being inauthentic).

**Responsibility:** Sartre emphasized that we are *condemned to be free*, meaning we cannot escape choosing, and with that comes responsibility for the outcomes. Not just for ourselves, but since by choosing we also implicitly affirm values (when I choose for myself I’m saying “this is good for me”, and Sartre provocatively said one should also consider if it’d be good if everyone did likewise – a kind of universality). Sartre’s idea is not fleshed out like Kant, but he suggests an awareness that when we create values we can’t help but imagine them as having a universal kind of worth (otherwise it’s hard to commit to them). Does an IU agent acknowledge any responsibility toward others? Likely not, except instrumentally. So existentialists might criticize IU for *denying the weight of freedom*: freedom is not just freedom *from* obligation; it’s also the burden of *deciding what is right* without guidelines. If IU just says “maximize your happiness,” it might be seen as an evasion of the harder task of figuring out what to value, by just defaulting to a simplistic formula.

On the flip side, existentialism has been criticized for not giving enough concrete moral guidance. IU certainly does give concrete guidance (even if one disagrees with it, it’s straightforward: maximize your well-being). So ironically IU could be seen as more practically prescriptive than existentialism. But it may come at the cost of arguably oversimplifying what one’s “well-being” entails – existentialists would say it’s not a pre-given metric, you actually shape what counts as your well-being by your values.

**Existential Individualism vs. Utilitarian Individualism:** Some forms of existentialism, like those influenced by Nietzsche or Stirner (Max Stirner was an egoist anarchist often considered a predecessor of existential individualism), celebrate the individual’s uniqueness and refusal to bow to general morality. Stirner’s work *The Ego and Its Own* (1844) advocated that the individual should act purely based on self-interest, rejecting all higher ideals as “spooks” in the mind. That is very much in line with IU’s content. Stirner considered even love or solidarity as valid only if they come from the ego’s will. This is perhaps the closest existential-ish philosophy to IU—though Stirner is sometimes classified as a radical egoist rather than existentialist, his influence on existentialism is noted. If we bring Stirner in: he wouldn’t mind IU at all; he’d likely applaud that one recognizes only the ego’s interest. But he might caution not to treat “utility” as some external measure—he would insist it’s whatever *you* will. IU’s idea of maximizing subjective well-being could be interpreted in Stirnerite fashion: “Do what *you* find fulfills you; don’t sacrifice to ideas of Good or Evil that are not yours.” In that sense, IU resonates with a very individualist existential stance.

**Summary:** Individual Utilitarianism and existentialism share an emphasis on the individual, but differ in nuance:
- **Freedom and Choice:** IU gives a specific content to one’s project (personal utility), whereas existentialists would say you have radical freedom to choose your project (which might or might not be centered on your own happiness).
- **Relation to Others:** IU and existentialism both acknowledge potential conflict and instrumentality in relations (Sartre’s object-subject struggle), but existentialists often push for an ethical transcendence of pure conflict (through recognizing the other’s freedom) as a part of authenticity, whereas IU stays in the paradigm of using others as means unless you choose otherwise for personal reasons.
- **Meaning vs. Happiness:** Existentialists focus on meaning, authenticity, and freedom often more than on happiness. IU focuses on happiness (subjective well-being). A deep existential critique of IU could be: pursuing happiness as a goal can be self-defeating (similar to the paradox of hedonism in psychology, where aiming directly at happiness can make it elusive). Victor Frankl (an existential psychiatrist) argued that meaning, not happiness, is what people really need, and happiness ensues as a byproduct. An IU might respond: then I’ll pursue meaning as part of my utility. But making it an “in order to be happy I’ll find meaning” is a subtle shift that might undermine genuine commitment. Existentialists might encourage pursuing something wholeheartedly for its own sake (which ironically is what some virtue ethicists or even deontologists say too), and happiness will follow indirectly; IU might short-circuit that process by keeping an eye on “Am I happy yet?”.

In conclusion, existentialism provides both a potential **justification** for IU (each person must choose their values, and one might choose to value oneself above all – that is one’s prerogative) and a **critique** (warning that such a choice might be inauthentic or shallow, and that denying the moral dimension of others’ freedom can lead to conflict and self-deception). Individual Utilitarianism can be seen as one existential choice among many – one that embraces individual well-being as life’s meaning. It is up to the individual to judge if that choice stands up to the test of authenticity and truly satisfies their quest for meaning.

This exploration of existentialism shows that IU is philosophically bold in centering the individual, but perhaps vulnerable to charges of superficiality or conflict with deeper human needs (like meaning or genuine connection). Next, we will turn to the psychological and behavioral sciences, moving from philosophical analysis to empirical and theoretical perspectives on how an IU approach might manifest in real cognition and behavior.

## Psychological and Behavioral Implications of Individual Utilitarianism

Individual Utilitarianism is not only a philosophical theory but also has implications for how an individual thinks, feels, and behaves. If a person genuinely adopts IU as their guiding framework, how might this influence their psychology? Conversely, what insights from psychology might support or undermine the assumptions of IU? In this section, we discuss links to cognitive-behavioral theory, decision science, and behavioral psychology—examining how an IU-oriented individual would handle emotions, make decisions, and possibly how such a mindset aligns with or contradicts human psychological tendencies.

### Subjective Well-Being and the Pursuit of Happiness

Since IU posits *subjective well-being (SWB)* as the ultimate value for the individual, understanding SWB is key. **Subjective well-being** is a term in psychology referring to how people experience and evaluate their lives, typically in terms of life satisfaction and balance of positive over negative affect (Diener, 1984). Research shows SWB has multiple components: life satisfaction (cognitive evaluation), positive affect, and negative affect (Diener, 2000). An IU practitioner, in effect, tries to maximize their SWB.

One immediate psychological question is the **Paradox of Hedonic Pursuit**: Does directly trying to maximize one’s happiness work, or can it backfire? Psychologists have found that chasing happiness too directly can sometimes make people less happy (Mauss et al., 2011). This might be because it sets unrealistic expectations and leads to constant self-monitoring (“Am I happy yet?”), which can reduce enjoyment. Effective happiness often comes as a byproduct of engaging in meaningful activities or social connections, rather than as a direct goal.

An IU agent could take this into account—since they are rational, they might realize “If I obsess about my happiness, I get anxious and unhappy, so instead I’ll focus on activities and values that tend to bring me happiness.” This is a subtlety: IU doesn’t necessarily imply a *constant conscious obsession* with utility; a smart IU agent could adopt habits and commitments that indirectly maximize happiness. For example, research suggests that people who practice gratitude or generosity often increase their own happiness (Lyubomirsky, 2007). Even if an IU agent doesn’t intrinsically care about others, they might practice kindness as a “life hack” to boost mood (since giving can produce a “helper’s high”). This would be a case of *instrumental altruism*: helping others to help oneself. Psychological studies on volunteering show it often benefits the volunteer’s mental health. An IU agent would cherry-pick such findings to structure their life.

**Emotional Regulation:** Cognitive-Behavioral Therapy (CBT) and specifically **Rational Emotive Behavior Therapy (REBT)** (Ellis, 1957) emphasize identifying and changing irrational beliefs that lead to unhealthy emotions (Ellis & Dryden, 1997). An IU agent has a strong reason to manage their emotions effectively: uncontrolled negative emotions detract from utility. So an IU person might be highly motivated to practice CBT techniques on themselves: disputing catastrophic thoughts, reframing situations positively, and cultivating a rational mindset that promotes emotional stability. For instance, suppose an IU agent experiences intense social anxiety (fear of others’ judgment) that prevents them from networking and getting career opportunities that would improve their happiness. Recognizing this, they might engage in CBT to reduce the anxiety—after all, the anxiety is a barrier to their utility. In this way, IU aligns with the idea of **emotional optimization**.

However, one could also worry that an IU agent might suppress or avoid emotions in unhealthy ways if they seek immediate comfort. But a far-sighted IU agent knows that unresolved emotional problems can fester and reduce long-term well-being. So ideally, they confront issues proactively (perhaps seeking therapy when needed) purely out of self-interest in mental well-being.

**Biases and Decision-Making:** Decision science and behavioral economics have documented many systematic biases in human decision-making (Kahneman & Tversky, 1974). For example, people often exhibit present bias (overvaluing immediate rewards at the expense of future well-being), or they might irrationally avoid known risks due to fear (even when payoff is positive in expectation). An IU agent, striving to maximize their own interest, would ideally want to overcome such biases. In classical rational choice terms, IU assumes an agent who can in principle follow expected utility maximization. Real humans deviate from that in ways that sometimes reduce their own well-being (like procrastination, addiction, etc.). 

Thus, a committed IU person might engage in strategies to counteract biases:
- **Precommitment:** Thomas Schelling (1980) discussed how individuals use commitment devices to achieve long-term goals (e.g., putting alarm clocks across the room, locking away temptations). An IU agent might, for instance, set up automatic savings plans or ask friends to hold them accountable to avoid short-term temptations that harm long-term utility (like overspending or overeating).
- **Self-Control as Investment:** Psychologically, delaying gratification (Mischel’s marshmallow test findings) is linked to better life outcomes. IU gives a *moral weight* to self-control: it’s not just prudent, it’s “moral” under IU to resist a small short-term pleasure that jeopardizes larger long-term happiness. So an IU agent has a quasi-ethical commitment to personal discipline. They might practice meditation or other techniques to strengthen self-control because it pays dividends in well-being (Tangney et al., 2004).

**Social Behavior of an IU Individual:** How would an IU follower behave socially? Likely in ways that maximize personal benefit, but ironically that can often mean being *nice, fair, and trustworthy*, due to the principle of reciprocity. Game theory tells us that in repeated interactions, cooperative strategies can yield higher personal payoff than defection (Axelrod, 1984). So an IU individual is incentivized to develop a good reputation, maintain relationships, and help others when there’s expectation of return or mutual gain. This overlaps significantly with what normal moral behavior looks like, except the motivation is different. Psychologically, though, motivation matters: if someone is friendly only instrumentally, could that affect the genuineness and thus effectiveness of their friendliness? Humans are good at sniffing out insincerity. An IU person might decide it’s beneficial to *actually* care (or convincingly simulate caring) in order to get the benefits. This is related to research on **emotions as commitment devices** (Frank, 1988). For example, genuinely feeling empathy might lead you to help even when nobody would find out (so it’s not obviously self-beneficial at that moment), but that capacity for empathy means people can trust you more, which *in the long run* is beneficial. So evolution may have given us some genuine pro-social emotions because they have long-run benefits. An IU person, understanding this, might *choose* to cultivate genuine empathy as a strategy (though some would argue you can’t just will genuine feelings purely for advantage; that’s the paradox: to get the benefit of genuine moral emotions, you kind of have to have them for real).

This aligns with **indirect reciprocity**: to be seen as generous might require sometimes acting generous when it doesn’t obviously pay off immediately, to build a reputation (Nowak & Sigmund, 2005). An IU individual among humans might thus engage in altruistic acts for the sake of being known as altruistic, which eventually circles back to them (either via reciprocation or status).

**Cognitive Dissonance:** If an IU person ever does something that doesn’t align with their self-interest (perhaps out of a spontaneous altruistic impulse), they might experience cognitive dissonance: “I believe I should always maximize my happiness, yet I just sacrificed for someone.” They might resolve this by either rationalizing the act as actually self-serving (“It made me feel good to do that, so it was fine”) or by adjusting their future behavior to be more consistently self-serving. This is interesting because cognitive dissonance theory (Festinger, 1957) suggests people want consistency in their beliefs and actions. If IU is their conscious philosophy, they will likely adjust either their interpretation of events or their future choices to keep consistent. In contrast, a person without that philosophy might feel fine occasionally doing selfless things without needing to justify it.

**Well-Being Research:** Positive psychology research suggests several drivers of long-term happiness: relationships, meaningful work, health, etc. (Diener & Seligman, 2002). An IU person might intensely study this literature as a “science of happiness” to guide their life choices. For instance, knowing that relationships are typically the top source of happiness for people, an IU agent would invest in friendships and family (not out of duty, but because it’s smart selfishness). They might also avoid common traps that reduce happiness, like materialistic excess or comparisons with others (since relative deprivation can lower happiness). That aligns with Stoic advice too – many ancient schools of thought like Stoicism or Epicureanism can be seen as proto-IU: Epicurus literally said the goal of life is one’s own pleasure (peaceful pleasure), and he advised on how to achieve that (valuing simple pleasures, friendship, avoiding political ambition which brings stress).

**Personality Considerations:** Is there a personality type more aligned with IU? Possibly high self-directedness and low agreeableness. People who are very high in *agreeableness* (Big Five trait) get a lot of satisfaction from harmony and often empathy; they might struggle with a purely self-oriented approach because they naturally feel others’ needs. People high in *narcissism* or *Machiavellianism* might find IU intuitive, but those traits have their own psychological issues (narcissists often actually have unstable self-esteem, Machiavellians may succeed in some realms but might have less fulfilling close relationships). An IU agent doesn’t have to be callous, but if one is too high in empathy, they might effectively value others intrinsically (which IU forbids except as part of one’s utility). So one might expect IU to appeal more to those who are comfortable with a level of detachment. Alternatively, a highly empathetic person could still be IU if they just accept that “this is how I get my happiness – by helping others” (so their empathy becomes their route to joy, still within IU if they define their well-being to include others’).

**Mental Health:** Does adopting IU affect mental health? Potentially both positively and negatively. Positively, focusing on one’s well-being can encourage self-care (proper diet, exercise, avoiding toxic situations), which is good. It can reduce guilt or martyrdom tendencies (some people sacrifice too much and burn out; an IU approach would tell them to step back and care for themselves – similar to advice in caregiver stress management). On the negative side, if taken too far, it might isolate a person or cause moral conflicts internally if they have a strong moral upbringing that conflicts with pure self-interest, leading to guilt or existential angst. Also, relationships might suffer if the person is perceived as selfish – loneliness would decrease happiness, ironically undermining IU goals.

Another angle is **moral development theory** (Kohlberg’s stages of moral development). Egoistic reasoning is usually seen as a lower stage (pre-conventional, focusing on punishment and reward to self). More advanced stages consider broader principles. If someone consciously stays in an egoistic mindset, one could say they are philosophically sophisticated but morally developmentally at a stage 2. However, this is a matter of perspective – IU would say “the higher stages that say consider others are just one viewpoint; I choose to stick to myself.”

**Cognitive Load:** Calculating everything for self-interest might be cognitively heavy in some situations, but we do have instincts. IU doesn’t require constant calculation if our intuitions (shaped by evolution and experience) reliably point toward self-interest. Many of our intuitions do: fear protects us, affection ties us to helpful others, anger can deter aggressors. But our intuitions were tuned for perhaps a different environment, and sometimes they misfire (e.g., anger leading to irrational aggression that actually harms one’s interest by creating enemies or legal troubles). So an IU person might sometimes override gut feelings if analysis shows they’re counterproductive. This is analogous to what a good CBT practitioner might do: “I feel like doing X out of anger, but that might hurt me more; let me calm down.” Essentially IU gives one a framework to override immediate impulses in favor of long-term planning.

**Altruism and Empathy in Psychology:** There’s debate in psychology about whether true altruism exists or if all acts have some egoistic motivation (C. Daniel Batson's empathy-altruism hypothesis vs. egoistic accounts). IU straightforwardly sides with: even if altruism exists, one *ought* not to follow it unless it aligns with self-interest. But psychologically, if empathy strongly compels a person to help, an IU agent might either avoid situations that trigger intense empathy (to not be compelled to sacrifice) or rationalize the help as making them feel better. There’s a phenomenon called *“empathic distress”* that people sometimes alleviate by helping. The IU agent will help to quell their distress – which looks altruistic externally, but inside it’s personal relief (Batson, 1991 discussed this as egoistic route to helping). So IU’s view maps onto one side of that debate: that many helps are really egoistic at root. Even if that’s not always true, an IU strategy might ironically involve cultivating enough empathy to ensure one does “good” actions that will rebound positively, but not so much empathy that one sacrifices beyond what returns as happiness.

### The Self and Identity

From a psychological perspective, the concept of *self* is complex. IU requires a fairly coherent, continuous notion of self whose welfare can be tracked. Humans have identity that evolves; what I value in my 20s may differ in my 40s. IU says maximize cumulative well-being, which implies caring for one’s future self. In psychology, caring for future self is related to “future self continuity” – if you feel connected to your future self, you make better long-term decisions (Hershfield et al., 2011). So IU basically encourages high future self continuity (treat “future me” like another person who I care about just as much as present me). Interestingly, some people treat their future self almost like a different person (hence procrastination or poor saving). IU morally obliges you to treat your future self’s utility as equal to present (maybe with slight discount if you rationally think future utility should be discounted, though most say from personal view, shouldn’t too much). 

So an IU believer might do visualization exercises to empathize with their future self (some studies have done MRI aged renderings to encourage saving). They might think: “Will 80-year-old me be glad I did this or not?”

However, what if one’s identity drastically changes (due to new values, or even something like memory loss)? Philosophically, if you took Parfit’s view that personal identity is not what matters (Parfit, 1984), then focusing only on “this self” might be questioned. But psychologically, we usually care about our future selves.

**Psychopathy and IU:** A potential concern – does IU encourage a psychopathic outlook (since psychopaths are often described as acting for personal gain without empathy)? Psychopathy in psychology is associated with lack of empathy and remorse, superficial charm, and often short-sighted impulsivity (Cleckley, 1941; Hare, 1991). IU, however, does not endorse short-sightedness; it requires rationality. A high-functioning psychopath might align with a short-term version of IU (just exploit and enjoy now). But a *rational* psychopath might do something like what we described – behave cooperatively when beneficial. Some research indicates “successful psychopaths” can channel their tendencies into socially acceptable ambition without criminality. IU might theoretically describe an idealized “rational sociopath” who carefully navigates society to maximize their gain (like some portrayals of certain corporate or political figures). 

However, just because IU and psychopathic morality both exclude intrinsic care for others doesn’t mean an IU follower would match a psychopath’s profile. They could be entirely pro-social in action because it yields better results. The difference is empathy: a psychopath doesn’t feel it, whereas an IU person could feel empathy but chooses to interpret it in self-regarding terms or moderate it. Actually, a psychopath lacks some emotional pieces which might ironically hinder long-term happiness (since deep relationships, love, etc., may be closed off to them). An IU agent might want love (because love often brings joy), whereas a psychopath may be incapable of love. So ironically, IU might endorse having capacity for love to enrich life, whereas psychopathy cannot go that route and might settle for power or shallow pleasures. 

**Ethical Training vs. Self-Help:** In some ways, IU could be presented as a form of *self-help ideology*: always do what is best for you (but figure out wisely what actually is best for you). It’s like a life coaching principle, minus any moral obligation to others. It could resonate with popular culture advice like “put on your own oxygen mask first” or “you can’t pour from an empty cup” – these are ways of telling people to ensure their own well-being so they can function well (often ultimately to help others, but also just for themselves). Many people struggle with guilt or neglecting themselves; IU if adopted would flip that – neglecting yourself would be the sin. This could be psychologically liberating for some, but for others it could license self-indulgence in a way that might harm them (e.g., someone might abandon a marriage or family because they think they’d be happier elsewhere, but later regret losing those connections; or someone might quit a job impulsively because it’s stressful, only to face worse stress unemployed). There’s a need for *wisdom* in applying IU.

**Overlap with Buddhism or Stoicism:** Interestingly, some philosophies like Stoicism or certain interpretations of Buddhism tell individuals to focus on their own enlightenment or tranquility. Stoics say virtue (rational living) is the only true good, but also it leads to the best life (tranquility). Buddhism says attachment causes suffering; by focusing on one’s own path to enlightenment, one paradoxically becomes more compassionate to others, but initially one is working on one’s own liberation. IU is more material in its concern (just happiness in a worldly sense, not necessarily virtue or enlightenment), but a person might adopt meditation, mindfulness, etc., as tools because they improve mental well-being. Mindfulness has been shown to reduce stress and improve emotional regulation. An IU agent may practice it daily to increase their happiness (indeed many people take up meditation for self-care reasons nowadays, even stripping it of its spiritual aspects). 

**Potential Pitfalls:** Some behavioral pitfalls for IU:
- **Short-termism:** if one isn’t careful, one might rationalize a lot of short-term pleasure (like “I’ll eat this cake because right now it increases my utility a lot” repeatedly) and then suffer later. A key challenge is maintaining a long horizon.
- **Over-optimization:** Life isn’t an equation we can fully solve. An IU person might fall into analysis paralysis or constant optimization attempts (like biohackers who meticulously measure diet, exercise, productivity, etc., to optimize output). This can become stressful or obsessive, ironically reducing happiness – the very Silicon Valley pursuit of optimal happiness sometimes causes anxiety.
- **Social alienation:** If others sense you don’t genuinely care, some friendships could be less intimate. People like to feel valued for their own sake. If an IU friend always does a cost-benefit analysis, it might come off as cold. While the IU person might still maintain friendships by being giving (because they know to keep friends, they must give and show care), if the mindset slips out (like they say or do something that reveals self-centered calculus), it could hurt trust. The IU individual might then have to weigh the benefit of honesty about their philosophy versus just hiding it and “pretending” to have normal moral impulses.

**In summary**, psychologically:
- IU promotes a very deliberate, self-regulated, and self-aware mode of life.
- It aligns with many evidence-based self-improvement strategies (CBT, mindfulness, planning).
- It may conflict with moral emotions (like empathy, guilt) unless those are reframed or moderated.
- It encourages forming positive habits and avoiding biases, essentially to become an optimal “Homo economicus” for one’s own life.
- It might risk issues with authenticity in relationships or a sense of emptiness if not balanced with meaning.

Overall, an IU approach in psychology looks like an extremely agentic, self-determined individual trying to engineer their life for maximum satisfaction. This has laudable aspects (who doesn’t want to be happier?) but also could become isolating or relentless, since you’re both the project and the project manager nonstop.

In the next section, we will gather the philosophical and practical threads and consider **vulnerabilities and critiques** of the IU model, some of which we’ve touched on (like unfalsifiability or reductionism), pulling them together systematically.
